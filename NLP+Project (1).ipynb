{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adb8921c"
      },
      "source": [
        "# Abstractive Text Summarization using Transformers\n",
        "#### Areeba Pervez Sharif \n",
        "#### 21I-2290 \n",
        "#### Course Project-Natural Language Processing "
      ],
      "id": "adb8921c"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4065a685"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle"
      ],
      "id": "4065a685"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e94f7dd4"
      },
      "source": [
        "#### Loading Data"
      ],
      "id": "e94f7dd4"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "b8af12af",
        "outputId": "06a1d253-c0b3-49a1-eccb-f4fe1a76f8fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           headlines  \\\n",
              "0                 single today ajay devgn jokes tabu   \n",
              "1  adani drops bn aus coal mine contract amid cas...   \n",
              "2        virat kohli applauds ms dhonis innings pune   \n",
              "3               shortage lakh police personnel india   \n",
              "4           apologise mistakes weaaave made uber ceo   \n",
              "\n",
              "                                                text  \n",
              "0  actress tabu jokingly said single today actor ...  \n",
              "1  adani group monday said cancelled billion cont...  \n",
              "2  india captain virat kohli applauded ms dhonis ...  \n",
              "3  union home ministry tuesday told parliament sh...  \n",
              "4  ncabhailing startup ubers ceo dara khosrowshah...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58d3a5a0-5181-4855-87c0-fb664cb58b73\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>single today ajay devgn jokes tabu</td>\n",
              "      <td>actress tabu jokingly said single today actor ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>adani drops bn aus coal mine contract amid cas...</td>\n",
              "      <td>adani group monday said cancelled billion cont...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>virat kohli applauds ms dhonis innings pune</td>\n",
              "      <td>india captain virat kohli applauded ms dhonis ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>shortage lakh police personnel india</td>\n",
              "      <td>union home ministry tuesday told parliament sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>apologise mistakes weaaave made uber ceo</td>\n",
              "      <td>ncabhailing startup ubers ceo dara khosrowshah...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58d3a5a0-5181-4855-87c0-fb664cb58b73')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-58d3a5a0-5181-4855-87c0-fb664cb58b73 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-58d3a5a0-5181-4855-87c0-fb664cb58b73');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "news = pd.read_csv(\"cleaned_data.csv\")\n",
        "\n",
        "news=news.drop(columns=['Unnamed: 0'])\n",
        "news.head()"
      ],
      "id": "b8af12af"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ceca0da",
        "outputId": "2385e3bf-0fef-4189-d667-aa0b3ab32558"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102915, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "news.shape"
      ],
      "id": "9ceca0da"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ed26beff"
      },
      "outputs": [],
      "source": [
        "document = news['text']\n",
        "summary = news['headlines']"
      ],
      "id": "ed26beff"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1db21f5f",
        "outputId": "caeedb9a-25c5-4096-8d57-c1beedd6c3cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('rthemthembering late pm jawaharlal nehru birth anniversary pm narendra modi wednesday wrote twitter recall contribution freedom struggle tenure pm delhi cm arvind kejriwal tweeted rthemthembering jawaharlal nehru first pm india laid foundation modern india',\n",
              " 'recall nehrus contribution freedom struggle pm modi')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "document[30], summary[30]"
      ],
      "id": "1db21f5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d1d9a61"
      },
      "source": [
        "# Pre-processing "
      ],
      "id": "4d1d9a61"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cd1656c",
        "outputId": "a038b0ce-4e1b-48d9-b14d-f621986fc984"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                  single today ajay devgn jokes tabu \n",
              "1     adani drops bn aus coal mine contract amid ca...\n",
              "2         virat kohli applauds ms dhonis innings pune \n",
              "3                shortage lakh police personnel india \n",
              "4            apologise mistakes weaaave made uber ceo \n",
              "Name: headlines, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# for decoder sequence\n",
        "summary = summary.apply(lambda x: ' ' + x + ' ')\n",
        "summary.head()"
      ],
      "id": "3cd1656c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49798ac0"
      },
      "source": [
        "### Tokenizing the text into integer tokens"
      ],
      "id": "49798ac0"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "134c0d48"
      },
      "outputs": [],
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = ''"
      ],
      "id": "134c0d48"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c0dfb355"
      },
      "outputs": [],
      "source": [
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n"
      ],
      "id": "c0dfb355"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b2d8d323"
      },
      "outputs": [],
      "source": [
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ],
      "id": "b2d8d323"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ff4bae11"
      },
      "outputs": [],
      "source": [
        "inputs = document_tokenizer.texts_to_sequences(document)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ],
      "id": "ff4bae11"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53f69233",
        "outputId": "d1a600ac-0229-4f2a-c5ab-242f7db37f46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 52]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "summary_tokenizer.texts_to_sequences([\"This is a test\"])"
      ],
      "id": "53f69233"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26f6921f",
        "outputId": "b2d76796-2a9e-42ab-8421-d43db3624786"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['state mumbai crore hc']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "summary_tokenizer.sequences_to_texts([[184, 22, 12, 71]])"
      ],
      "id": "26f6921f"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a44ce18a",
        "outputId": "8d3897d5-13a8-4259-e53f-1bca0280bbc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['   test']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "summary_tokenizer.sequences_to_texts([[1,1,1,52]])"
      ],
      "id": "a44ce18a"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6969fb9c",
        "outputId": "f6422141-4f35-4b02-80e6-752d3a5588d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(107137, 39758)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "id": "6969fb9c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "598dec5b"
      },
      "source": [
        "#### Obtaining insights on lengths for defining maxlen"
      ],
      "id": "598dec5b"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "12d9df4e"
      },
      "outputs": [],
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "id": "12d9df4e"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f57989fe",
        "outputId": "47bca864-1bdc-4cb6-d897-e436f889bc88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    102915.000000\n",
              "mean        254.929301\n",
              "std          32.265250\n",
              "min           4.000000\n",
              "25%         233.000000\n",
              "50%         255.000000\n",
              "75%         278.000000\n",
              "max         374.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "document_lengths.describe()"
      ],
      "id": "f57989fe"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcf6c989",
        "outputId": "1dee49d1-e659-4971-e6a3-18dadfd36d6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    102915.000000\n",
              "mean         48.640130\n",
              "std           6.590938\n",
              "min          11.000000\n",
              "25%          45.000000\n",
              "50%          49.000000\n",
              "75%          53.000000\n",
              "max          74.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "summary_lengths.describe()"
      ],
      "id": "bcf6c989"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b0a29ff3"
      },
      "outputs": [],
      "source": [
        "# maxlen\n",
        "# taking values > and round figured to 75th percentile\n",
        "# at the same time not leaving high variance\n",
        "encoder_maxlen = 50\n",
        "decoder_maxlen = 10"
      ],
      "id": "b0a29ff3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8a58201"
      },
      "source": [
        "### Padding/Truncating sequences for identical sequence lengths"
      ],
      "id": "f8a58201"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eb84ba1c"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
      ],
      "id": "eb84ba1c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bae4e3a"
      },
      "source": [
        "#### Creating dataset pipeline"
      ],
      "id": "0bae4e3a"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0ead8b16"
      },
      "outputs": [],
      "source": [
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ],
      "id": "0ead8b16"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "52606312"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "id": "52606312"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "f61f11cc"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "id": "f61f11cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "550919da"
      },
      "source": [
        "### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional"
      ],
      "id": "550919da"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "63d11cd1"
      },
      "outputs": [],
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "id": "63d11cd1"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "999dd465"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "id": "999dd465"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c1bf51e"
      },
      "source": [
        "### Masking\n",
        "#### 1. Padding mask for masking \"pad\" sequences\n",
        "#### 2. Lookahead mask for masking future words from contributing in prediction of current words in self attention"
      ],
      "id": "2c1bf51e"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cbc73839"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "id": "cbc73839"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "d06b47eb"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "id": "d06b47eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ccad49c"
      },
      "source": [
        "#### Building the Model\n",
        "##### Scaled Dot Product"
      ],
      "id": "7ccad49c"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3d326169"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "id": "3d326169"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "557b2f09"
      },
      "source": [
        "#### Multi-Headed Attention"
      ],
      "id": "557b2f09"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cf865d9a"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "id": "cf865d9a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ba8c74f"
      },
      "source": [
        "#### Feed forward network"
      ],
      "id": "0ba8c74f"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "b0ba2172"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "id": "b0ba2172"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f25c2f7"
      },
      "source": [
        "#### Fundamental Unit of Transformer encoder"
      ],
      "id": "6f25c2f7"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "b8dae9c5"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "id": "b8dae9c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2cc6507"
      },
      "source": [
        "#### Fundamental Unit of Transformer decoder"
      ],
      "id": "f2cc6507"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "d3ea9577"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "id": "d3ea9577"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da59c1df"
      },
      "source": [
        "#### Encoder consisting of multiple EncoderLayer(s)"
      ],
      "id": "da59c1df"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "b529f912"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "id": "b529f912"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55c29671"
      },
      "source": [
        "#### Decoder consisting of multiple DecoderLayer(s)"
      ],
      "id": "55c29671"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "507c159b"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights"
      ],
      "id": "507c159b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "362989aa"
      },
      "source": [
        "### Finally, the Transformer"
      ],
      "id": "362989aa"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "f383b05a"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "id": "f383b05a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5233d79"
      },
      "source": [
        "### Training"
      ],
      "id": "a5233d79"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "63fff794"
      },
      "outputs": [],
      "source": [
        "# hyper-params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 10"
      ],
      "id": "63fff794"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54f58526"
      },
      "source": [
        "#### Adam optimizer with custom learning rate scheduling"
      ],
      "id": "54f58526"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "914c7ab7"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "id": "914c7ab7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1f8ba18"
      },
      "source": [
        "#### Defining losses and other metrics"
      ],
      "id": "c1f8ba18"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2420fef8"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "id": "2420fef8"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8314f706"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "id": "8314f706"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5d8517f5"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "id": "5d8517f5"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "996cf888"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "id": "996cf888"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ba89b14"
      },
      "source": [
        "#### Transformer"
      ],
      "id": "2ba89b14"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4c2956e8"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "id": "4c2956e8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb1fa28c"
      },
      "source": [
        "#### Masks"
      ],
      "id": "eb1fa28c"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "45ce87b4"
      },
      "outputs": [],
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "id": "45ce87b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99f639b1"
      },
      "source": [
        "#### Checkpoints"
      ],
      "id": "99f639b1"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "b911a046"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "id": "b911a046"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14cbe003"
      },
      "source": [
        "### Training steps"
      ],
      "id": "14cbe003"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "e50e09d5"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "id": "e50e09d5"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e28546",
        "outputId": "c740b07d-bc43-44d7-ac6f-54952d0bdf41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 10.5917\n",
            "Epoch 1 Batch 805 Loss 9.3637\n",
            "Epoch 1 Loss 8.8946\n",
            "Time taken for 1 epoch: 1493.80082321167 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 8.1525\n",
            "Epoch 2 Batch 805 Loss 7.8449\n",
            "Epoch 2 Loss 7.6686\n",
            "Time taken for 1 epoch: 1480.5535652637482 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 7.2961\n",
            "Epoch 3 Batch 805 Loss 7.1711\n",
            "Epoch 3 Loss 7.0666\n",
            "Time taken for 1 epoch: 1480.6020839214325 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 6.8291\n",
            "Epoch 4 Batch 805 Loss 6.6485\n",
            "Epoch 4 Loss 6.5747\n",
            "Time taken for 1 epoch: 1502.0835027694702 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 6.4023\n",
            "Epoch 5 Batch 805 Loss 6.2435\n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
            "Epoch 5 Loss 6.2025\n",
            "Time taken for 1 epoch: 1524.1866810321808 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 6.3638\n",
            "Epoch 6 Batch 805 Loss 5.9693\n",
            "Epoch 6 Loss 5.9631\n",
            "Time taken for 1 epoch: 1518.8512246608734 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 5.5245\n",
            "Epoch 7 Batch 805 Loss 5.8249\n",
            "Epoch 7 Loss 5.8309\n",
            "Time taken for 1 epoch: 1514.8323955535889 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 5.6012\n",
            "Epoch 8 Batch 805 Loss 5.6962\n",
            "Epoch 8 Loss 5.7014\n",
            "Time taken for 1 epoch: 1519.2198023796082 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 5.8582\n",
            "Epoch 9 Batch 805 Loss 5.5588\n",
            "Epoch 9 Loss 5.5687\n",
            "Time taken for 1 epoch: 1525.5908296108246 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 5.3792\n",
            "Epoch 10 Batch 805 Loss 5.4373\n",
            "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
            "Epoch 10 Loss 5.4577\n",
            "Time taken for 1 epoch: 1526.4734055995941 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        # 103k samples\n",
        "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "        # 103k / 64 ~ 1610; 1610 / 2 = 805\n",
        "        if batch % 805 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "id": "a2e28546"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "f14360de"
      },
      "outputs": [],
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "id": "f14360de"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "05d4cb26"
      },
      "outputs": [],
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing  token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "id": "05d4cb26"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "bc43a066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7ae82a4a-89ec-4dad-c6a7-dd4cd0100b82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gets biggest deal firm billionaire dies aged deal deal deal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "summarize(\"Pernod has reduced the debt it took on to fund the Seagram purchase to just 1.8bn euros, while Allied has improved the performance of its fast-food chains.Shares in UK drinks and food firm Allied Domecq have risen on speculation that it could be the target of a takeover by France's Pernod Ricard.Pernod said it was seeking acquisitions but refused to comment on specifics.In terms of market value, Pernod - at 7.5bn euros ($9.7bn) - is about 9% smaller than Allied Domecq, which has a capitalisation of £5.7bn ($10.7bn; 8.2bn euros).Allied Domecq shares in London rose 4% by 1200 GMT, while Pernod shares in Paris slipped 1.2%.\")"
      ],
      "id": "bc43a066"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "cjZwHk5-k1QF"
      },
      "outputs": [],
      "source": [],
      "id": "cjZwHk5-k1QF"
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}